{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258fc436",
   "metadata": {},
   "source": [
    "# Project Galilei\n",
    "### \"Measure what is measurable, and make measurable what is not so.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d41ad2",
   "metadata": {},
   "source": [
    "## Attention Model Abstractive Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f29dc0",
   "metadata": {},
   "source": [
    "    import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87994796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec, callbacks\n",
    "\n",
    "from modules import preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Attention, Dropout, Input, Embedding,Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fbed5f",
   "metadata": {},
   "source": [
    "    read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fe2fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_meta = pd.read_json(\"../data/raw/arxiv-metadata-oai-snapshot.json\", lines=True, nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9551aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>versions</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0001</td>\n",
       "      <td>Pavel Nadolsky</td>\n",
       "      <td>C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>37 pages, 15 figures; published version</td>\n",
       "      <td>Phys.Rev.D76:013009,2007</td>\n",
       "      <td>10.1103/PhysRevD.76.013009</td>\n",
       "      <td>ANL-HEP-PR-07-12</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>None</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
       "      <td>2008-11-26</td>\n",
       "      <td>[[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0002</td>\n",
       "      <td>Louis Theran</td>\n",
       "      <td>Ileana Streinu and Louis Theran</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>To appear in Graphs and Combinatorics</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
       "      <td>2008-12-13</td>\n",
       "      <td>[[Streinu, Ileana, ], [Theran, Louis, ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>704.0003</td>\n",
       "      <td>Hongjun Pan</td>\n",
       "      <td>Hongjun Pan</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>23 pages, 3 figures</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>physics.gen-ph</td>\n",
       "      <td>None</td>\n",
       "      <td>The evolution of Earth-Moon system is descri...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sun, 1 Apr 2007...</td>\n",
       "      <td>2008-01-13</td>\n",
       "      <td>[[Pan, Hongjun, ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>704.0004</td>\n",
       "      <td>David Callan</td>\n",
       "      <td>David Callan</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>11 pages</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CO</td>\n",
       "      <td>None</td>\n",
       "      <td>We show that a determinant of Stirling cycle...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
       "      <td>2007-05-23</td>\n",
       "      <td>[[Callan, David, ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704.0005</td>\n",
       "      <td>Alberto Torchinsky</td>\n",
       "      <td>Wael Abu-Shammala and Alberto Torchinsky</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>None</td>\n",
       "      <td>Illinois J. Math. 52 (2008) no.2, 681-689</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "      <td>None</td>\n",
       "      <td>In this paper we show how to compute the $\\L...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
       "      <td>2013-10-15</td>\n",
       "      <td>[[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id           submitter  \\\n",
       "0  704.0001      Pavel Nadolsky   \n",
       "1  704.0002        Louis Theran   \n",
       "2  704.0003         Hongjun Pan   \n",
       "3  704.0004        David Callan   \n",
       "4  704.0005  Alberto Torchinsky   \n",
       "\n",
       "                                             authors  \\\n",
       "0  C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...   \n",
       "1                    Ileana Streinu and Louis Theran   \n",
       "2                                        Hongjun Pan   \n",
       "3                                       David Callan   \n",
       "4           Wael Abu-Shammala and Alberto Torchinsky   \n",
       "\n",
       "                                               title  \\\n",
       "0  Calculation of prompt diphoton production cros...   \n",
       "1           Sparsity-certifying Graph Decompositions   \n",
       "2  The evolution of the Earth-Moon system based o...   \n",
       "3  A determinant of Stirling cycle numbers counts...   \n",
       "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                  comments  \\\n",
       "0  37 pages, 15 figures; published version   \n",
       "1    To appear in Graphs and Combinatorics   \n",
       "2                      23 pages, 3 figures   \n",
       "3                                 11 pages   \n",
       "4                                     None   \n",
       "\n",
       "                                 journal-ref                         doi  \\\n",
       "0                   Phys.Rev.D76:013009,2007  10.1103/PhysRevD.76.013009   \n",
       "1                                       None                        None   \n",
       "2                                       None                        None   \n",
       "3                                       None                        None   \n",
       "4  Illinois J. Math. 52 (2008) no.2, 681-689                        None   \n",
       "\n",
       "          report-no       categories  \\\n",
       "0  ANL-HEP-PR-07-12           hep-ph   \n",
       "1              None    math.CO cs.CG   \n",
       "2              None   physics.gen-ph   \n",
       "3              None          math.CO   \n",
       "4              None  math.CA math.FA   \n",
       "\n",
       "                                             license  \\\n",
       "0                                               None   \n",
       "1  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                            abstract  \\\n",
       "0    A fully differential calculation in perturba...   \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-...   \n",
       "2    The evolution of Earth-Moon system is descri...   \n",
       "3    We show that a determinant of Stirling cycle...   \n",
       "4    In this paper we show how to compute the $\\L...   \n",
       "\n",
       "                                            versions update_date  \\\n",
       "0  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  2008-11-26   \n",
       "1  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  2008-12-13   \n",
       "2  [{'version': 'v1', 'created': 'Sun, 1 Apr 2007...  2008-01-13   \n",
       "3  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  2007-05-23   \n",
       "4  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  2013-10-15   \n",
       "\n",
       "                                      authors_parsed  \n",
       "0  [[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...  \n",
       "1           [[Streinu, Ileana, ], [Theran, Louis, ]]  \n",
       "2                                 [[Pan, Hongjun, ]]  \n",
       "3                                [[Callan, David, ]]  \n",
       "4  [[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_meta.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ebb814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_text = arxiv_meta[[\"id\", \"title\", \"abstract\", \"authors_parsed\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e30dd7",
   "metadata": {},
   "source": [
    "    preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db966c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab set of stopwords from nltk and CV\n",
    "sw = set(stopwords.words(\"english\"))\n",
    "cv = set(CountVectorizer(stop_words=\"english\").get_stop_words())\n",
    "\n",
    "# union of stopwrods from nltk and CV extended with custom stopwords\n",
    "stops = list(set(cv | sw))\n",
    "stops.extend([\"php\", \"k\", \"ell\", \"n\", \"r\", \"t\", \"bf\", \"pi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e883abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjwjl\\AppData\\Local\\Temp/ipykernel_23732/1881454821.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  arxiv_text[\"processed_abstract\"] = preprocessing.free_form_preprocessor(arxiv_text, column_name=\"abstract\", stops = stops)\n",
      "C:\\Users\\mjwjl\\AppData\\Local\\Temp/ipykernel_23732/1881454821.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  arxiv_text[\"processed_title\"] = preprocessing.free_form_preprocessor(arxiv_text, column_name=\"title\", stops = [\"php\", \"k\", \"ell\", \"n\", \"r\", \"t\", \"bf\", \"pi\"])\n"
     ]
    }
   ],
   "source": [
    "arxiv_text[\"processed_abstract\"] = preprocessing.free_form_preprocessor(arxiv_text, column_name=\"abstract\", stops = stops)\n",
    "arxiv_text[\"processed_title\"] = preprocessing.free_form_preprocessor(arxiv_text, column_name=\"title\", stops = [\"php\", \"k\", \"ell\", \"n\", \"r\", \"t\", \"bf\", \"pi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8b6a3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjwjl\\AppData\\Local\\Temp/ipykernel_23732/1995339452.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  arxiv_text[\"processed_title\"] = [\"<SOS> \" + x + \" <EOS>\" for x in arxiv_text[\"processed_title\"]]\n"
     ]
    }
   ],
   "source": [
    "# https://arxiv.org/pdf/1409.3215.pdf\n",
    "# From what I understand it is debatable whether these are truly necessary. They're featured in many\n",
    "# of the plate diagrams I reviewed so I include them here.\n",
    "arxiv_text[\"processed_title\"] = [\"<SOS> \" + x + \" <EOS>\" for x in arxiv_text[\"processed_title\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0eb36f",
   "metadata": {},
   "source": [
    "### Tokenize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01040fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract_tokens = [RegexpTokenizer(r\"\\w+\").tokenize(x) for x in arxiv_text[\"processed_abstract\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d21e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_tokens = [gensim.utils.simple_preprocess(x, deacc=True) for x in arxiv_text[\"processed_abstract\"]]\n",
    "title_tokens = [gensim.utils.simple_preprocess(x, deacc=True) for x in arxiv_text[\"processed_title\"]]\n",
    "# tokens = [gensim.utils.simple_preprocess(x, deacc=True) for x in arxiv_text[\"processed_title\"]+arxiv_text[\"processed_abstract\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71702d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "longest_abstract = max([len(abstract_tokens[i]) for i in range(len(abstract_tokens))])\n",
    "longest_title = max([len(title_tokens[i]) for i in range(len(title_tokens))])\n",
    "print(longest_abstract)\n",
    "print(longest_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df37d9f",
   "metadata": {},
   "source": [
    "This post led me down a wierd path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fa8818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This post led me down a wierd path\n",
    "# # https://stackoverflow.com/questions/42064690/using-pre-trained-word2vec-with-lstm-for-word-generation\n",
    "# X_train = [gensim.utils.simple_preprocess(x, deacc=True) for x in X_train]\n",
    "# X_test = [gensim.utils.simple_preprocess(x, deacc=True) for x in X_test]\n",
    "# y_train = [gensim.utils.simple_preprocess(x, deacc=True) for x in y_train]\n",
    "# y_test = [gensim.utils.simple_preprocess(x, deacc=True) for x in y_test]\n",
    "\n",
    "# X_train_zeroes = np.zeros([len(X_train),\n",
    "#                            max([len(X_train[i]) for i in range(len(X_train))])])\n",
    "# y_train_zeroes = np.zeros([len(y_train),\n",
    "#                            max([len(y_train[i]) for i in range(len(y_train))])])\n",
    "\n",
    "# for i, abstract in enumerate(X_train):\n",
    "#     for j, token in enumerate(abstract):\n",
    "#         X_train_zeroes = skip_embeddings.wv.key_to_index[token]\n",
    "\n",
    "# #X, y = (arxiv_text[\"processed_abstract\"], arxiv_text[\"processed_title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354f49a5",
   "metadata": {},
   "source": [
    "The right way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20e59fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(arxiv_text[\"processed_abstract\"], arxiv_text[\"processed_title\"], test_size=.2, random_state=137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4641dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.Tokenizer(num_words=10000)\n",
    "tokens.fit_on_texts(pd.concat([X_train, X_test, y_train, y_test]))\n",
    "\n",
    "X_train = tokens.texts_to_sequences(X_train)\n",
    "X_test = tokens.texts_to_sequences(X_test)\n",
    "y_train = tokens.texts_to_sequences(y_train)\n",
    "y_test = tokens.texts_to_sequences(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "21733800",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = tokens.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8dec97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding and use pretrained Word2vec instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfcc8d",
   "metadata": {},
   "source": [
    "### Embed Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef381dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The documentation for gensim is hit or miss. In this case the implementation of built in callbacks\n",
    "# is obfuscated and possibly broken\n",
    "# https://stackoverflow.com/questions/54888490/gensim-word2vec-print-log-loss\n",
    "\n",
    "class callback(callbacks.CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.loss_to_be_subbed = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_to_be_subbed\n",
    "        self.loss_to_be_subbed = loss\n",
    "        print(f\"Loss after epoch {self.epoch}: {loss_now}\")\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43da3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip_embeddings = Word2Vec(sentences=tokens, vector_size=128, alpha=0.02, window=15, min_count=5,\n",
    "#                            workers=4, sg=1, max_vocab_size=25000, negative=5,\n",
    "#                            epochs=100, compute_loss=True, callbacks=[callback()])\n",
    "\n",
    "# skip_embeddings.save(\"../models/embeddings/skip_embedding_nlg.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb5490f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip_embeddings = Word2Vec(sentences=tokens, vector_size=128, alpha=0.02, window=15,\n",
    "#                            workers=4, sg=1, max_vocab_size=25000, negative=5,\n",
    "#                            epochs=100, compute_loss=True, callbacks=[callback()])\n",
    "\n",
    "# skip_embeddings.save(\"../models/embeddings/skip_embedding_nlg_no_min.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "072b77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbow_embeddings = Word2Vec(sentences=tokens, vector_size=128, alpha=0.02, window=15,\n",
    "#                            workers=4, sg=0, max_vocab_size=25000, negative=5,epochs=100,\n",
    "#                            compute_loss=True, callbacks=[callback()])\n",
    "\n",
    "# cbow_embeddings.save(\"../models/embeddings/cbow_embedding_nlg.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9809fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_embeddings = Word2Vec.load(\"../models/embeddings/skip_embedding_nlg.model\")\n",
    "\n",
    "skip_embeddings_no_min = Word2Vec.load(\"../models/embeddings/skip_embedding_nlg_no_min.model\")\n",
    "\n",
    "cbow_embedding = Word2Vec.load(\"../models/embeddings/cbow_embedding_nlg.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ff765",
   "metadata": {},
   "source": [
    "### Encoder-Decoder with Attention Model: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b899582",
   "metadata": {},
   "source": [
    "I am confident this will run through by the end of the week. Just need to homogenize/pad inputs and adjust input/encoder-decoder shapes accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b767c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Ten Minute Intro to seq2seq learning in keras\n",
    "# https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "\n",
    "# Adapted from examples in keras documentation:\n",
    "# https://keras.io/examples/generative/lstm_character_level_text_generation/\n",
    "# https://keras.io/examples/nlp/text_generation_fnet/\n",
    "# https://keras.io/examples/generative/text_generation_with_miniature_gpt/\n",
    "\n",
    "# embeddings can be used as weights\n",
    "# https://stackoverflow.com/questions/50311253/how-to-interpret-shape-of-word2vec-weights\n",
    "\n",
    "encoder_input = Input(shape=(200, ))\n",
    "\n",
    "embedding = Embedding(input_dim=,\n",
    "                      output_dim=skip_embeddings.wv.vectors.shape[1],\n",
    "                      input_length= 200, weights=[skip_embeddings.wv.vectors])(encoder_input)\n",
    "\n",
    "encoder_layer_1 = LSTM(256, name=\"LSTM_1\", return_state=True, recurrent_dropout=.1)#, return_sequences=True)\n",
    "encoder_layer_1_output, e_h_1, e_c_1 = encoder_layer_1(embedding)\n",
    "state_vector_1 = [e_h_1, e_c_1]\n",
    "\n",
    "encoder_layer_2 = LSTM(256, name=\"LSTM_2\", return_state=True, recurrent_dropout=.1)\n",
    "encoder_layer_2_output, e_h_1, e_c_1 = encoder_layer_2(encoder_layer_1_output)\n",
    "state_vector_2 = [e_h_2, e_c_2]\n",
    "\n",
    "encoder_layer_3 = LSTM(256, name=\"LSTM_3\", return_state=True, recurrent_dropout=.1)\n",
    "encoder_layer_3_output, e_h_1, e_c_1 = encoder_layer_3(encoder_layer_2_output)\n",
    "state_vector_3 = [e_h_3, e_c_3]\n",
    "\n",
    "encoder_layer_4 = LSTM(256, name=\"LSTM_4\", return_state=True, recurrent_dropout=.1)\n",
    "encoder_layer_4_output, e_h_1, e_c_1 = encoder_layer_4(encoder_layer_3_output)\n",
    "state_vector_4 = [e_h_4, e_c_4]\n",
    "\n",
    "decoder_input = Input(shape=(200, ))\n",
    "\n",
    "decoder = LSTM(256, name=\"decoder_LSTM\", return_state=True, return_sequences=True)\n",
    "decoder_outputs, _, _ = decoder(decoder_input, intial_state=state_vector_4)\n",
    "\n",
    "# https://keras.io/api/layers/attention_layers/attention/\n",
    "attention_head = Attention()\n",
    "attention_dist, attention_value = attention_head([encoder_layer_4_output, decoder_outputs])\n",
    "\n",
    "concat_output = Concatenate(axis=-1)\n",
    "dec_att_out = concat_output([decoder_outputs, attention_dist])\n",
    "\n",
    "y_hat = Dense(200, activation=\"softmax\")(dec_att_out)\n",
    "\n",
    "attention_model = Model([encoder_input, decoder_input], y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb27cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/api/losses/probabilistic_losses/#sparse_categorical_crossentropy-function\n",
    "attention_model.compile(optimizer=\"sgd\", loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a7fe7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(encoder_decoder, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cbe463",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_model.fit([encoder_input, decoder_input], decoder_target, batch_size=512, epochs=10,\n",
    "                    validation_split=.2)\n",
    "\n",
    "es = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience = 5, mode=\"auto\")\n",
    "\n",
    "history = attention_model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                       epochs=10, batch_size=100, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"Training\")\n",
    "plt.plot(history.history[\"val_loss\"] label=\"Test\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1468a5d2",
   "metadata": {},
   "source": [
    "### Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5e84b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
